{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiDn2Sk-VWqE"
      },
      "outputs": [],
      "source": [
        "# Run this to use from colab environment\n",
        "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1kxzuvBYl0e"
      },
      "source": [
        "## Data loading and cleaning\n",
        "\n",
        "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NE_fTbKGe5z",
        "outputId": "41d8a998-c9c9-49c0-c753-ce6b03f90d93"
      },
      "outputs": [],
      "source": [
        "#Load packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt \n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import datasets as ds\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Classifiers --> later bijwerken welke we gebruiken\n",
        "# from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "# Importing Data loading functions. Uncomment the one you want to use\n",
        "from worcliver.load_data import load_data\n",
        "\n",
        "from scipy.stats import ttest_ind, mannwhitneyu, shapiro\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, cross_val_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the data\n",
        "data = load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Presentation of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Description of the data\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n",
        "# Display basic info\n",
        "print(\"First five rows of the dataset:\")\n",
        "print(data.head(), \"\\n\")\n",
        "\n",
        "print(\"Dataset info:\")\n",
        "print(data.info(), \"\\n\")\n",
        "\n",
        "print(\"Summary statistics:\")\n",
        "print(data.describe(), \"\\n\")\n",
        "\n",
        "# Counting missing values\n",
        "missing_values = data.isnull().sum()\n",
        "total_missing = missing_values.sum()\n",
        "\n",
        "print(f\"Total missing values in the dataset: {total_missing}\\n\")\n",
        "print(f\"Missing values per column:\\n{missing_values[missing_values > 0]}\\n\")\n",
        "\n",
        "# Counting categorical and numerical columns\n",
        "categorical_columns = data.select_dtypes(include=['object']).columns\n",
        "numerical_columns = data.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "print(f\"Number of categorical columns: {len(categorical_columns)}\")\n",
        "print(f\"Number of numerical columns: {len(numerical_columns)}\\n\")\n",
        "\n",
        "# Count of each label (benign/malignant)\n",
        "label_counts = data['label'].value_counts()\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "print(label_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# For loop with a 4 subplots in which 2 features are put against eachother to visualize the data\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "for i in range(4):\n",
        "    random_columns = np.random.choice(data.columns[:-1], size=2, replace=False)\n",
        "    \n",
        "    # Use the index number of the feature for the title\n",
        "    feature_num1 = list(data.columns).index(random_columns[0]) + 1  # Adding 1 to make it human-readable (starting from 1)\n",
        "    feature_num2 = list(data.columns).index(random_columns[1]) + 1  # Adding 1 to make it human-readable (starting from 1)\n",
        "    \n",
        "    ax = fig.add_subplot(2, 2, i + 1)  # Manually creating subplots (2 rows, 2 columns)\n",
        "    sns.scatterplot(x=data[random_columns[0]], y=data[random_columns[1]], hue=data['label'], palette='coolwarm', ax=ax)\n",
        "    ax.set_title(f\"Scatter plot between Feature {feature_num1} and Feature {feature_num2}\")\n",
        "    ax.set_xlabel(f\"Feature {feature_num1}\")\n",
        "    ax.set_ylabel(f\"Feature {feature_num2}\")\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Changing labels malignant and benign to 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#changing string (malignant or benign) to 0 or 1\n",
        "num_data = data.copy()\n",
        "\n",
        "# Transform labels: benign -> 1, malignant -> 0\n",
        "num_data['label'] = num_data['label'].map({'benign': 1, 'malignant': 0})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Splitting data in train and test test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Splitting data in test and train set \n",
        "y = num_data['label']\n",
        "x = num_data[:]\n",
        "\n",
        "#x is features, y  = maligne / benign\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "#print the shape of the data sets  \n",
        "print(f'The shape of the train data: {x_train.shape}')\n",
        "print(f'The shape of the test data: {x_test.shape}')\n",
        "\n",
        "x_train.to_csv('x_train.csv', index=False) \n",
        "x_test.to_csv('x_test.csv', index=False)\n",
        "y_train.to_csv('y_train.csv', index=False) \n",
        "y_test.to_csv('y_test.csv', index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Selecting significant features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select significant features\n",
        "# Add the label column back to x_train\n",
        "x_train_with_label = x_train.copy()\n",
        "x_train_with_label['label'] = y_train\n",
        "\n",
        "# Separate benign and malignant samples in training data\n",
        "benign = x_train_with_label[x_train_with_label['label'] == 1]\n",
        "malignant = x_train_with_label[x_train_with_label['label'] == 0]\n",
        "\n",
        "x_train_with_label.to_csv('x_train_label.csv', index=False) \n",
        "# Perform statistical analysis\n",
        "features = []\n",
        "sig_features = []\n",
        "\n",
        "# Loop through all features except 'label'\n",
        "for feature in x_train_with_label.columns:\n",
        "    if feature == 'label':  # Skip the label column\n",
        "        continue\n",
        "        \n",
        "    # Data for current feature\n",
        "    benign_values = benign[feature].dropna()\n",
        "    malignant_values = malignant[feature].dropna()\n",
        "    \n",
        "    # Normality test (Shapiro-Wilk test, p < 0.05 means not normally distributed)\n",
        "    _, p_benign = shapiro(benign_values)\n",
        "    _, p_malignant = shapiro(malignant_values)\n",
        "    \n",
        "    # Determine which test to use\n",
        "    if p_benign > 0.05 and p_malignant > 0.05:  # Both distributions are normal\n",
        "        test_type = \"t-test\"\n",
        "        stat, p_value = ttest_ind(benign_values, malignant_values, equal_var=False)  # Welch's t-test\n",
        "    else:\n",
        "        test_type = \"Mann-Whitney U-test\"\n",
        "        stat, p_value = mannwhitneyu(benign_values, malignant_values, alternative='two-sided')\n",
        "    \n",
        "    # Save results\n",
        "    feature_entry = {\n",
        "        \"Feature\": feature,\n",
        "        \"Test\": test_type,\n",
        "        \"p_value\": p_value\n",
        "    }\n",
        "    features.append(feature_entry)\n",
        "    \n",
        "    # Check for significance (p <= 0.05)\n",
        "    if p_value <= 0.05:\n",
        "        sig_entry = feature_entry.copy()\n",
        "        # sig_entry['benign_mean'] = benign_values.mean()\n",
        "        # sig_entry['malignant_mean'] = malignant_values.mean()\n",
        "        # sig_entry['benign_std'] = benign_values.std()\n",
        "        # sig_entry['malignant_std'] = malignant_values.std()\n",
        "        sig_features.append(sig_entry)\n",
        "\n",
        "# Convert to DataFrames\n",
        "features_df = pd.DataFrame(features)\n",
        "sig_features_df = pd.DataFrame(sig_features)\n",
        "\n",
        "# Multiple testing correction (False Discovery Rate - Benjamini-Hochberg)\n",
        "if not features_df.empty:\n",
        "    reject, p_corrected, alphacSidak, alphacBonf = multipletests(features_df[\"p_value\"], method='fdr_bh')\n",
        "    features_df[\"p_value_corrected\"] = p_corrected\n",
        "\n",
        "# Sort significant features by p-value only if there are significant features\n",
        "if not sig_features_df.empty:\n",
        "    sig_features_df = sig_features_df.sort_values(by=\"p_value\")\n",
        "\n",
        "# Print and save results\n",
        "# print(\"\\nTotal Significant Features:\")\n",
        "# print(sig_features_df)\n",
        "print(f\"\\nNumber of significant features: {len(sig_features_df)}\")\n",
        "\n",
        "# Get the list of significant feature names\n",
        "sig_feature_names = sig_features_df['Feature'].tolist() if not sig_features_df.empty else []\n",
        "\n",
        "# Select only significant features for training data if there are any\n",
        "if sig_feature_names:\n",
        "    x_train_sig = x_train[sig_feature_names].copy()\n",
        "    # Save results to CSV\n",
        "    x_train_sig.to_csv('x_train_sig.csv', index=False)\n",
        "else:\n",
        "    print(\"No significant features found. Cannot create x_train_sig.\")\n",
        "\n",
        "# Save results to CSV\n",
        "# features_df.to_csv('features.csv', index=False)\n",
        "sig_features_df.to_csv('sig_features_df.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear classifier\n",
        "# Test how many feautures are normal distributed and covariances equal (p > 0.05)\n",
        "\n",
        "#From features_df, sum how many t test used, i.e. are normal distributed from all features\n",
        "normal_distributed = features_df.loc[features_df['Test'] == 't-test', 'Test'].count() \n",
        "print(f' The ammount of normal distributed features is: {normal_distributed}')\n",
        "\n",
        "#the ammount of normal distibuted from significant features \n",
        "normal_distributed_sig = sig_features_df.loc[features_df['Test'] == 't-test', 'Test'].count() \n",
        "normal_distributed_non_sig = normal_distributed - normal_distributed_sig\n",
        "print(f' The ammount of normal distributed features (p>0.05) {normal_distributed_non_sig}')\n",
        "\n",
        "#Percentage of usable featrues for lineair classifier\n",
        "print(f' Percentages of usable features for lineair classifier: {normal_distributed_non_sig/(len(data.columns))*100}')\n",
        "\n",
        "\n",
        "#hier wordt de lineaire classif gemaakt, bepalen of we dat willen met 52\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "lda = lda.fit(x_train, y_train)\n",
        "y_pred_lda = lda.predict(x_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quadratic classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quadratic classifier\n",
        "# Test how many feautures are normal distributed and covariances not equal (p < 0.05)\n",
        "\n",
        "#From features_df, sum how many t test used, i.e. are normal distributed from all features\n",
        "normal_distributed = features_df.loc[features_df['Test'] == 't-test', 'Test'].count() \n",
        "print(f' The ammount of normal distributed features is: {normal_distributed}')\n",
        "\n",
        "#the ammount of normal distibuted from non significant features \n",
        "normal_distributed_sig = sig_features_df.loc[features_df['Test'] == 't-test', 'Test'].count() \n",
        "print(f' The ammount of normal distributed features (p < 0.05) is: {normal_distributed_sig}')\n",
        "\n",
        "#Percentage of usable featrues for quadratic classifier\n",
        "print(f' Percentages of usable features for lineair classifier: {normal_distributed_sig/(len(data.columns))*100}')\n",
        "\n",
        "# quadratic uitvoeren, later wss weg\n",
        "#q_clas = QuadraticDiscriminantAnalysis()\n",
        "#q_clas = q_clas.fit(x_train, y_train)\n",
        "#q_clas_pred = q_clas.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Voorbeeld functie loss funcite (voor lda) --> niet werkend op lda, code later aanpassen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for lda, X1, Y1 in zip(lda, x_train, y_train):\n",
        "auc=metrics.roc_auc_score(y_train, y_pred_lda)\n",
        "accuracy=metrics.accuracy_score(y_train, y_pred_lda)\n",
        "F1=metrics.f1_score(y_train,y_pred_lda)\n",
        "precision=metrics.precision_score(y_train,y_pred_lda)\n",
        "recall=metrics.recall_score(y_train, y_pred_lda)\n",
        "\n",
        "    # accuracy, AUC, f1score, precision, recall\n",
        "print(type(lda))\n",
        "print('Acc:' +str(accuracy))\n",
        "print('AUC:' +str(auc))\n",
        "print('F1:' +str(F1))\n",
        "print('precision:' +str(precision))\n",
        "print('recall:' +str(recall))\n",
        "\n",
        "for learning_rate in [0.001, 0.01, 0.1, 0.5]:\n",
        "\n",
        "    print(f\"Training with learning rate {learning_rate}\")\n",
        "    print(\"Training set score: %f\" % lda.score(x_train, y_train))\n",
        "    print(\"Test set score: %f\" % lda.score(x_test, y_test))\n",
        "\n",
        "    plt.plot(lda.loss_curve_, label=f'lr = {learning_rate}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Support vector machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Support vector machine ('hard)\n",
        "\n",
        "svm_linear = SVC(kernel='linear', C=1e10)\n",
        "validation_svm_linear = cross_val_score(estimator=svm_linear, X=x_train, y=y_train, cv= 4)\n",
        "\n",
        "\n",
        "#svm_linear.fit(x_train, y_train)\n",
        "#y_train_pred = svm_linear.predict(x_train)\n",
        "#y_test_pred = svm_linear.predict(x_test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Soft margin support vector machine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random forest classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# random forest code\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=10, criterion='gini', min_samples_split=2)\n",
        "#rf.fit(x_train, y_train)\n",
        "validation_rf = cross_val_score(estimator=rf, X=x_train, y=y_train, cv= 4)\n",
        "print(validation_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Calculate mean and std of individual classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Decription \n",
        "## Dit nog aanpassen naar berekenen op basis van 'label' ipv deze nieuwe csv files\n",
        "# Mean\n",
        "benign_mean = benign.mean(numeric_only=True)\n",
        "malignant_mean = malignant.mean(numeric_only=True)\n",
        "\n",
        "# Standard deviation\n",
        "benign_std = benign.std(numeric_only=True)\n",
        "malignant_std = malignant.std(numeric_only=True)\n",
        "\n",
        "#Print statistics\n",
        "benign_stats = pd.DataFrame({\n",
        "    'mean': benign.mean(numeric_only=True),\n",
        "    'std': benign.std(numeric_only=True)\n",
        "})\n",
        "print(benign_stats)\n",
        "benign_stats.T.to_csv('benign_stats.csv')\n",
        "\n",
        "\n",
        "malignant_stats = pd.DataFrame({\n",
        "    'mean': malignant.mean(numeric_only=True),\n",
        "    'std': malignant.std(numeric_only=True)\n",
        "})\n",
        "malignant_stats.T.to_csv('malignant_stats.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# k-NN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Checking skewness for each feature in the training set in order to choose the scaler\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m skewness \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mskew()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Printing the skewness for each feature\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkewness of features:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Checking skewness for each feature in the training set in order to choose the scaler\n",
        "skewness = x_train.skew()\n",
        "\n",
        "# Printing the skewness for each feature\n",
        "print(\"Skewness of features:\")\n",
        "print(skewness)\n",
        "\n",
        "# Identifying features that have skewness greater than 1 or less than -1 (highly skewed)\n",
        "skewed_features = skewness[abs(skewness) > 1].index\n",
        "print(\"\\nHighly skewed features (Skewness > 1 or < -1):\")\n",
        "print(skewed_features)\n",
        "\n",
        "# Checking for outliers using Z-scores (Threshold: Z > 3 or Z < -3)\n",
        "z_scores = np.abs(zscore(x_train))\n",
        "\n",
        "# Identifying features with Z-scores above the threshold (outliers)\n",
        "outliers = (z_scores > 3)\n",
        "\n",
        "# Getting the feature names where outliers exist\n",
        "outlier_features = x_train.columns[(outliers.any(axis=0))].tolist()\n",
        "print(\"\\nFeatures with outliers (Z-score > 3):\")\n",
        "print(outlier_features)\n",
        "\n",
        "# Optionally, calculate the IQR for outlier detection\n",
        "Q1 = x_train.quantile(0.25)\n",
        "Q3 = x_train.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Detecting outliers based on IQR\n",
        "outliers_iqr = ((x_train < (Q1 - 1.5 * IQR)) | (x_train > (Q3 + 1.5 * IQR)))\n",
        "\n",
        "# Getting feature names with IQR outliers\n",
        "outlier_features_iqr = x_train.columns[(outliers_iqr.any(axis=0))].tolist()\n",
        "print(\"\\nFeatures with outliers based on IQR:\")\n",
        "print(outlier_features_iqr)\n",
        "\n",
        "# Making the classifier\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', RobustScaler()),\n",
        "    ('pca', PCA()),\n",
        "    ('knn', KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'pca__n_components': [5, 10, 15, 20, 25],\n",
        "    'knn__n_neighbors': list(range(1, 31, 2)),  # Odd k values from 1 to 29\n",
        "    'knn__weights': ['uniform', 'distance'],\n",
        "    'knn__metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "\n",
        "# Perform hyperparameter optimization\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Best model evaluation\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(x_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and test accuracy\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "### PLOT 1: Learning Curve (Apparent vs. True Error)\n",
        "best_k = grid_search.best_params_['knn__n_neighbors']\n",
        "best_pca = grid_search.best_params_['pca__n_components']\n",
        "\n",
        "# Define the final best pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', RobustScaler()),  \n",
        "    ('pca', PCA(n_components=best_pca)),  \n",
        "    ('knn', KNeighborsClassifier(n_neighbors=best_k, metric='euclidean', weights='uniform'))\n",
        "])\n",
        "\n",
        "# Generate learning curve data\n",
        "train_sizes, train_scores, val_scores = learning_curve(\n",
        "    pipeline, x_train, y_train, cv=5, scoring='accuracy',\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "# Compute mean and standard deviation\n",
        "train_mean = np.mean(train_scores, axis=1)\n",
        "train_std = np.std(train_scores, axis=1)\n",
        "val_mean = np.mean(val_scores, axis=1)\n",
        "val_std = np.std(val_scores, axis=1)\n",
        "\n",
        "# Convert accuracy to error (1 - accuracy)\n",
        "train_error = 1 - train_mean\n",
        "val_error = 1 - val_mean\n",
        "\n",
        "# Plot Apparent Error (Training) vs True Error (Validation)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_error, marker='o', color='blue', label='Apparent Error (Training)')\n",
        "plt.plot(train_sizes, val_error, marker='o', color='red', label='True Error (Validation)')\n",
        "plt.fill_between(train_sizes, train_error - train_std, train_error + train_std, color='blue', alpha=0.2)\n",
        "plt.fill_between(train_sizes, val_error - val_std, val_error + val_std, color='red', alpha=0.2)\n",
        "\n",
        "# Labels and Title\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('Error (1 - Accuracy)')\n",
        "plt.title('Learning Curve: Apparent vs. True Error')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "### PLOT 2: Accuracy vs. k\n",
        "k_values = list(range(1, 31, 2))  # Odd k values from 1 to 29\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Loop through different k values\n",
        "for k in k_values:\n",
        "    # Define the pipeline with current k\n",
        "    pipeline = Pipeline([\n",
        "        ('scaler', RobustScaler()),  \n",
        "        ('pca', PCA(n_components=best_pca)),  \n",
        "        ('knn', KNeighborsClassifier(n_neighbors=k, metric='euclidean', weights='uniform'))\n",
        "    ])\n",
        "    \n",
        "    # Perform cross-validation\n",
        "    scores = cross_val_score(pipeline, x_train, y_train, cv=5, scoring='accuracy')\n",
        "    \n",
        "    # Fit on training data to get training accuracy\n",
        "    pipeline.fit(x_train, y_train)\n",
        "    train_accuracy = accuracy_score(y_train, pipeline.predict(x_train))\n",
        "\n",
        "    # Store accuracies\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(scores.mean())\n",
        "\n",
        "# Plot training vs validation accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, train_accuracies, marker='o', linestyle='-', label='Training Accuracy', color='blue')\n",
        "plt.plot(k_values, val_accuracies, marker='o', linestyle='-', label='Validation Accuracy', color='red')\n",
        "\n",
        "# Labels and Title\n",
        "plt.xlabel(\"Number of Neighbors (k)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs. k for k-NN Classifier\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
