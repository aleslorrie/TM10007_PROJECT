{"cells":[{"cell_type":"markdown","metadata":{"id":"mkDS3z2Nfr2w"},"source":["# TM10007: Machine learning\n","## Week 4, lecture 1: Neural networks\n","#### Author: Karin A. van Garderen/ Muhammad Arif\n","For this exercise, we will learn to work with the basic neural network that is implemented in scikit-learn:\n","[MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"V2MJIHPRfr2y"},"source":["## Preparing the datasset\n","We will be using the MNIST dataset, containing handwritten digits in 28 by 28 pixel images. In this case, each pixel is a feature and the labels are the numbers 0 through 9.\n","This part of the exercise was extended from [this example provided by scikit-learn](https://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html#sphx-glr-auto-examples-neural-networks-plot-mnist-filters-py)\n","\n","First, let's download the data, split it in a train and test set and show some examples."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0b4Lv1oNfr2z"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","\n","\n","# Load data from https://www.openml.org/d/554\n","X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n","X = X / 255.0\n","\n","# Split data into train partition and test partition\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.7)\n","\n","# Plot a few examples\n","fig, axes = plt.subplots(4, 4)\n","for sample, ax in zip(X_train[0:16], axes.ravel()):\n","    ax.matshow(sample.reshape(28, 28), cmap=plt.cm.gray, vmin=0, vmax=1)\n","    ax.set_xticks(())\n","    ax.set_yticks(())\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fq8yHy28fr2z"},"source":["## Training the network\n","We will be using the neural network classifier implemented in scikit-learn, called the Multi-layer Perceptron. This class takes care of all the complexities of optimizing the network, while allowing some modifications through hyperparameters. It can handle any number of inputs and outputs, and will adapt the number of connections accordingly.\n","\n","### Network design\n","First of all, we can design the network architecture by assigning the number of hidden layers and the number of neurons (hidden features) in those layers. Simply input a tuple of as many layers as you want, with the number of neurons in each layer.\n","\n","Let's start with a network that has no hidden layers at all. As the output is directly linked to the input, the network will have 784 x 10 = 7840 weights. We can still visualise those very easily, because they represent the weights of all the pixels in predicting each of the output digits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQIH4SsIfr20"},"outputs":[],"source":["from sklearn.exceptions import ConvergenceWarning\n","from sklearn.neural_network import MLPClassifier\n","\n","## We set the learning rate relatively high so that we get our results quickly\n","mlp = MLPClassifier(\n","        hidden_layer_sizes=(),\n","        learning_rate_init=0.01\n","    )\n","\n","mlp.fit(X_train, y_train)\n","\n","print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n","print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n","\n","fig, axes = plt.subplots(2, 5)\n","# use global min / max to ensure all weights are shown on the same scale\n","vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n","\n","print(f\"Number of weights: {mlp.coefs_[0].size}\")\n","for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n","    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n","    ax.set_xticks(())\n","    ax.set_yticks(())\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RCOQBYgdfr21"},"source":["- Can you recognize all the digits?\n","- What do you notice about the distribution of the weights?\n","- What does a very high value mean? Or a very low value?\n","- And what about the grey (in-between) values?"]},{"cell_type":"markdown","metadata":{"id":"LhAbGz4efr21"},"source":["## Adding hidden layers\n","Now let's add some hidden layers and see what happens. The hidden layer can form intermediate features, in order to model more complex relations between the input and output. We can still plot the weights of the first hidden layer as images, but they are no longer directly linked to the 10 digits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-F4T-44Lfr22"},"outputs":[],"source":["print(\"Training a neural network with 40 hidden features.\")\n","\n","mlp = MLPClassifier(\n","        hidden_layer_sizes=(40,),\n","        learning_rate_init=0.01\n","    )\n","\n","mlp.fit(X_train, y_train)\n","\n","print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n","print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n","\n","# Plot the weights of the first layer again\n","fig, axes = plt.subplots(1, 5)\n","vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n","for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n","    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n","    ax.set_xticks(())\n","    ax.set_yticks(())\n","plt.show()\n","\n","print(\"Training a neural network with two layers of 40 hidden features.\")\n","\n","mlp = MLPClassifier(\n","        hidden_layer_sizes=(40,40),\n","        learning_rate_init=0.01\n","    )\n","\n","mlp.fit(X_train, y_train)\n","\n","print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n","print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n","\n","# Plot the weights of the first layer again\n","fig, axes = plt.subplots(1, 5)\n","vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max()\n","for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()):\n","    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=0.5 * vmin, vmax=0.5 * vmax)\n","    ax.set_xticks(())\n","    ax.set_yticks(())\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"C4WksdVmfr23"},"source":["- How has the distribution of the weights changed?\n","- Can you still recognize digits in these images?\n","- Does adding this hidden layer also improve the performance?"]},{"cell_type":"markdown","metadata":{"id":"T3yKsnwGfr23"},"source":["## Learning rate\n","Next, let's have a look at the influence of the learning rate. As you know, the learning rate defines the size of the steps in each iteration of the gradient descent optimizer. A higher learning rate might get you to the optimal solution faster, but it might also cause noisy behavior and a worse final solution.\n","\n","To investigate the convergence of the optimization, we can plot the loss at each step or 'epoch'. Let's do that for a variation of learning rates and see what happens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WT3RWbsHfr23"},"outputs":[],"source":["for learning_rate in [0.001, 0.01, 0.1, 0.5]:\n","\n","    print(f\"Training with learning rate {learning_rate}\")\n","    ## To prevent very long waiting times, we set a maximum number of iterations\n","    mlp = MLPClassifier(\n","        hidden_layer_sizes=(40,),\n","        max_iter=100,\n","        alpha=1e-4,\n","        solver=\"sgd\",\n","        random_state=1,\n","        learning_rate_init=learning_rate\n","    )\n","\n","\n","    mlp.fit(X_train, y_train)\n","\n","    print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n","    print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n","\n","    plt.plot(mlp.loss_curve_, label=f'lr = {learning_rate}')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"EctbmhFnfr24"},"source":["- From these results, can you tell what is the best learning rate for this problem?\n","- Do you expect this to be the same across different problems and networks?"]},{"cell_type":"markdown","metadata":{"id":"fA1sfHRzfr24"},"source":["## Generalization\n","From the loss curve it seems that our neural network can get a near perfect performance, but how will it perform on unseen data? By tracking the accuracy on the test set for each epoch, we can get see how the actual performance changes during training. By comparing the accuracy of training and test set, we can get a sense of the degree of overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4wcWkLefr24"},"outputs":[],"source":["mlp = MLPClassifier(\n","    hidden_layer_sizes=(100),\n","    random_state=1,\n","    learning_rate_init=0.01\n",")\n","\n","# The MLPClassifier doesn't automatically log the accuracies during training,\n","# so we will step through the optimization manually and log the performance\n","# at each step.\n","train_scores = []\n","test_scores = []\n","n = 40\n","\n","for i in range(0,n):\n","    mlp.partial_fit(X_train, y_train, np.unique(y_train))\n","    train_scores.append(mlp.score(X_train, y_train))\n","    test_scores.append(mlp.score(X_test, y_test))\n","\n","plt.plot(train_scores, label='Training accuracy')\n","plt.plot(test_scores, label='Test accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"V9DicBpUfr24"},"source":["- Would you consider this classifier overfitted? Why (not)?"]},{"cell_type":"markdown","metadata":{"id":"ujDrVnfPfr25"},"source":["## Regularization\n","To illustrate the mechanisms of overfitting and regularization in neural networks, we will use a dataset with fewer samples and a higher dimensionality. For this we will use the text dataset that was introduced in the previous exercise, this time with 2000 extracted features.\n","\n","The `MLPClassifier` implemented in Scikit-learn allows for L2 regularization using the `alpha` parameter. This encourages the weights to become smaller, in theory limiting the potential for overfitting. We will try this with some extreme values of `alpha` and track the accuracy and loss.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H-tVQy3Mfr25"},"outputs":[],"source":["### Code to load and transform the dataset\n","\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import HashingVectorizer\n","from sklearn.feature_selection import SelectKBest, chi2\n","from time import time\n","\n","def load_text_dataset(N_features=100):\n","    '''\n","    Load dataset for classifying text documents by topic.\n","    '''\n","    categories = [\n","        'alt.atheism',\n","        'talk.religion.misc'\n","    ]\n","\n","    remove = ('headers', 'footers', 'quotes')\n","\n","    print(\"Loading 20 newsgroups dataset for categories:\")\n","    print(categories if categories else \"all\")\n","\n","    data_train = fetch_20newsgroups(subset='train', categories=categories,\n","                                    shuffle=True, random_state=42,\n","                                    remove=remove)\n","\n","    data_test = fetch_20newsgroups(subset='test', categories=categories,\n","                                   shuffle=True, random_state=42,\n","                                   remove=remove)\n","    print('data loaded')\n","\n","    # order of labels in `target_names` can be different from `categories`\n","    target_names = data_train.target_names\n","\n","\n","    def size_mb(docs):\n","        return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n","\n","\n","    data_train_size_mb = size_mb(data_train.data)\n","    data_test_size_mb = size_mb(data_test.data)\n","\n","    print(\"%d documents - %0.3fMB (training set)\" % (\n","        len(data_train.data), data_train_size_mb))\n","    print(\"%d documents - %0.3fMB (test set)\" % (\n","        len(data_test.data), data_test_size_mb))\n","    print(\"%d categories\" % len(target_names))\n","    print()\n","\n","    # split a training set and a test set\n","    y_train, y_test = data_train.target, data_test.target\n","\n","    print(\"Extracting features from the training data using a sparse vectorizer\")\n","    t0 = time()\n","    use_hashing = False\n","    if use_hashing:\n","        vectorizer = HashingVectorizer(stop_words='english', alternate_sign=False,\n","                                       n_features=2 ** 16)\n","        X_train = vectorizer.transform(data_train.data)\n","    else:\n","        vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n","                                     stop_words='english')\n","        X_train = vectorizer.fit_transform(data_train.data)\n","    duration = time() - t0\n","    print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n","    print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n","    print()\n","\n","    print(\"Extracting features from the test data using the same vectorizer\")\n","    t0 = time()\n","    X_test = vectorizer.transform(data_test.data)\n","    duration = time() - t0\n","    print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n","    print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n","    print()\n","\n","    # mapping from integer feature name to original token string\n","    if use_hashing:\n","        feature_names = None\n","    else:\n","        #feature_names = vectorizer.get_feature_names()\n","        feature_names = vectorizer.get_feature_names_out()\n","\n","    if N_features < X_train.shape[1]:\n","        print(\"Extracting %d best features by a chi-squared test\" %\n","              N_features)\n","        t0 = time()\n","        ch2 = SelectKBest(chi2, k=N_features)\n","        X_train = ch2.fit_transform(X_train, y_train)\n","        X_test = ch2.transform(X_test)\n","        # if feature_names:\n","        if len(feature_names) != 0:\n","            # keep selected feature names\n","            feature_names = [feature_names[i] for i\n","                             in ch2.get_support(indices=True)]\n","        print(\"done in %fs\" % (time() - t0))\n","        print()\n","\n","    if feature_names:\n","        feature_names = np.asarray(feature_names)\n","\n","    return X_train, X_test, y_train, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7kGDiHZfr26"},"outputs":[],"source":["X_train_text, X_test_text, y_train_text, y_test_text = load_text_dataset(N_features=2000)\n","\n","# Prepare the plots\n","fig, axes = plt.subplots(1, 3, figsize=(20,10))\n","# Number of epochs\n","n=200\n","\n","# Train neural networks with different values\n","# of the regularization parameter alpha\n","for alpha in [0, 0.1, 1]:\n","    print(f'Training with alpha = {alpha}')\n","    mlp = MLPClassifier(\n","            hidden_layer_sizes=(100),\n","            alpha=alpha,\n","            random_state=1,\n","            learning_rate_init=0.001\n","        )\n","\n","    train_scores = []\n","    test_scores = []\n","\n","    # Track the train and test performances for every training step\n","    for i in range(0,n):\n","        mlp.partial_fit(X_train_text, y_train_text, np.unique(y_train_text))\n","        train_scores.append(mlp.score(X_train_text, y_train_text))\n","        test_scores.append(mlp.score(X_test_text, y_test_text))\n","\n","    axes[0].plot(test_scores, label=f'Alpha = {alpha}')\n","    axes[1].plot(train_scores, label=f'Alpha = {alpha}')\n","    axes[2].plot(mlp.loss_curve_, label=f'Alpha = {alpha}')\n","\n","axes[0].set_xlabel('Epoch')\n","axes[1].set_xlabel('Epoch')\n","axes[2].set_xlabel('Epoch')\n","axes[0].set_ylabel('Test accuracy')\n","axes[1].set_ylabel('Train accuracy')\n","axes[2].set_ylabel('Train loss')\n","axes[0].legend()\n","axes[1].legend()\n","axes[2].legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"78X2JmwEfr26"},"source":["- Does regularization improve the performance for this classifier?\n","- How would you find the optimal value for alpha?\n","- Can you think of other ways to reduce overfitting?"]},{"cell_type":"markdown","metadata":{"id":"h3mH4_5Zfr26"},"source":["## Early stopping\n","When training neural networks we sometimes notice that overfitting gets worse as we keep training, and the performance on the test set starts decreasing. A simple way to improve performance is to look back at the loss curve and select the parameters where the performance on a validation set is at the highest point.\n","\n","The MLPClassifier can implement such a scheme, called 'Early stopping', and will automatically keep a part of the training set separate to track the validation loss. Let's see if this will improve performance for the network we were training before"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2BTcwwEefr27"},"outputs":[],"source":["print(\"Without early stopping.\")\n","\n","mlp = MLPClassifier(\n","        hidden_layer_sizes=(100),\n","        alpha=0,\n","        random_state=1,\n","        learning_rate_init=0.001,\n","        early_stopping=False\n","    )\n","\n","mlp.fit(X_train_text, y_train_text)\n","\n","print(\"Training set score: %f\" % mlp.score(X_train_text, y_train_text))\n","print(\"Test set score: %f\" % mlp.score(X_test_text, y_test_text))\n","\n","print(\"With early stopping.\")\n","\n","mlp = MLPClassifier(\n","        hidden_layer_sizes=(100),\n","        alpha=0,\n","        random_state=1,\n","        learning_rate_init=0.001,\n","        early_stopping=True\n","    )\n","\n","mlp.fit(X_train_text, y_train_text)\n","\n","print(\"Training set score: %f\" % mlp.score(X_train_text, y_train_text))\n","print(\"Test set score: %f\" % mlp.score(X_test_text, y_test_text))\n"]},{"cell_type":"markdown","metadata":{"id":"P06p9bVJfr27"},"source":["- Did the performance improve as much as you had expected?\n","- Can you think of a reason not to use early stopping?\n","- Why do we need a separate validation set? What would happen if we used the test set for early stopping?"]},{"cell_type":"markdown","metadata":{"id":"gvETwjRLfr27"},"source":["## Conclusion\n","The MLPClassifier offers a simple implementation of a neural network for classification. Have a look at the documentation to see all the hyperparameters, because\n","we did not cover them all.\n","\n","Remember that the optimal network architecture and hyperparameters may vary a lot between datasets. When designing your network, it is worthwhile to log the loss curve, both for the training and validation/test set, because it will give some insight in the behavior of the network.\n","Questions to consider:\n","- Is the optimization converging to slowly, or is there a lot of noise?\n","- Is the network overfitting? If so, you could try to reduce the network size or apply regularization.\n","\n","Remember though: we would expect some difference between the train and test performance, and this is not always a reason to be concerned!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}