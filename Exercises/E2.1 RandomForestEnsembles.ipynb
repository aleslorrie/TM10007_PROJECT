{"cells":[{"cell_type":"markdown","metadata":{"id":"A_QPNzp0gM0H"},"source":["# TM10007: Machine learning\n","## Week 2, lecture 1: Random Forest and Ensembles\n","#### Author: Martijn P. A. Starmans\n","\n","In this exercise, you will learn how to use a random forest and ensembles using scikit learn.\n","\n","For more ensemble methods, see https://scikit-learn.org/stable/modules/ensemble.html .\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMxkvj70rPMi"},"outputs":[],"source":["# General packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets as ds\n","\n","# Classifiers\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.ensemble import VotingClassifier\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndDvb37erIk8"},"outputs":[],"source":["# Some functions we will use\n","from sklearn.decomposition import PCA\n","\n","def colorplot(clf, ax, x, y, h=100):\n","    '''\n","    Overlay the decision areas as colors in an axes.\n","\n","    Input:\n","        clf: trained classifier\n","        ax: axis to overlay color mesh on\n","        x: feature on x-axis\n","        y: feature on y-axis\n","        h(optional): steps in the mesh\n","    '''\n","    # Create a meshgrid the size of the axis\n","    xstep = (x.max() - x.min() ) / 20.0\n","    ystep = (y.max() - y.min() ) / 20.0\n","    x_min, x_max = x.min() - xstep, x.max() + xstep\n","    y_min, y_max = y.min() - ystep, y.max() + ystep\n","    h = max((x_max - x_min, y_max - y_min))/h\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                         np.arange(y_min, y_max, h))\n","\n","    # Plot the decision boundary. For that, we will assign a color to each\n","    # point in the mesh [x_min, x_max]x[y_min, y_max].\n","    if hasattr(clf, \"decision_function\"):\n","        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n","    else:\n","        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n","    if len(Z.shape) > 1:\n","        Z = Z[:, 1]\n","\n","    # Put the result into a color plot\n","    cm = plt.cm.RdBu_r\n","    Z = Z.reshape(xx.shape)\n","    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n","    del xx, yy, x_min, x_max, y_min, y_max, Z, cm\n","\n","def load_breast_cancer(n_features=2):\n","    '''\n","    Load the sklearn breast data set, but reduce the number of features with PCA.\n","    '''\n","    data = ds.load_breast_cancer()\n","    x = data['data']\n","    y = data['target']\n","\n","    p = PCA(n_components=n_features)\n","    p = p.fit(x)\n","    x = p.transform(x)\n","    return x, y\n","\n","def load_boston(n_features=1):\n","    '''\n","    Load the sklearn boston data set, but reduce the number of features with PCA.\n","    '''\n","    data = ds.load_boston()\n","    x = data['data']\n","    y = data['target']\n","\n","    p = PCA(n_components=n_features)\n","    p = p.fit(x)\n","    x = p.transform(x)\n","    return x, y\n","\n","def load_diabetes(n_features=1):\n","    '''\n","    Load the sklearn bdiabetes data set, but reduce the number of features with PCA.\n","    '''\n","    data = ds.load_diabetes()\n","    x = data['data']\n","    y = data['target']\n","\n","    p = PCA(n_components=n_features)\n","    p = p.fit(x)\n","    x = p.transform(x)\n","    return x, y"]},{"cell_type":"markdown","metadata":{"id":"X941i8dqgM0M"},"source":["Let us first create again three example datasets to play with and plot the feature distributions\n","in scatter plots."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2F8gyQ0rIk-"},"outputs":[],"source":["# Load and plot three different classification datasets\n","X2, Y2 = ds.make_classification(n_samples=100, n_features=2, n_redundant=0,\n","                                n_informative=1,\n","                                n_clusters_per_class=1)\n","fig = plt.figure(figsize=(24,8))\n","ax = fig.add_subplot(131)\n","ax.set_title(\"One informative feature, one cluster per class\", fontsize='small')\n","ax.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","\n","X3, Y3 = ds.make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=5)\n","ax = fig.add_subplot(132)\n","ax.set_title(\"Two blobs, two classes\", fontsize='small')\n","ax.scatter(X3[:, 0], X3[:, 1], marker='o', c=Y3,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","\n","X4, Y4 = load_breast_cancer()\n","ax = fig.add_subplot(133)\n","ax.set_title(\"A more complicated problem\", fontsize='small')\n","ax.scatter(X4[:, 0], X4[:, 1], marker='o', c=Y4,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)"]},{"cell_type":"markdown","metadata":{"id":"6qw4VAo2gM0M"},"source":["Previously, we have seen how sklearn can be used to fit and test a classifier on these datasets using both\n","decision trees and a nearest neighbors classifiers. Now, we are going to see how we can use and evaluate\n","random forest and ensemble approaches.\n","\n","**Random Forest**\n","A random forest mostly consists of bagging of multiple decision trees. As sklearns provides a BaggingClassifier,\n","we can manually replicate a random forest using this approach. Note that you can in general use bagging with any classifier.\n","\n","**Ensembling**\n","We have seen multiple ensembling strategies, including voting. Sklearn provides a voting ensemble through the\n","VotingClassifier object, in which you can insert any number of any classifiers you want. You can use hard\n","voting to give a binary outcome, or soft voting to give a probability score, see the documentation\n","\n","Let's compare the previously used classifiers with our new classifiers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CPd2U_0ArIlB"},"outputs":[],"source":["# Construct classifiers\n","homemade_random_forest = BaggingClassifier(DecisionTreeClassifier())\n","voting_ensemble = VotingClassifier(\n","    estimators=[('KNN', KNeighborsClassifier()), ('tree', DecisionTreeClassifier()), ('rf', RandomForestClassifier())],\n","    voting='soft')\n","clsfs = [KNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier(),\n","         homemade_random_forest, voting_ensemble]\n","\n","\n","# Create lists of datasets to loop over\n","Xs = [X2, X3, X4]\n","Ys = [Y2, Y3, Y4]\n","\n","# First make plot without classifiers:\n","num = 0\n","fig = plt.figure(figsize=(24,8*len(clsfs)))\n","for X, Y in zip(Xs, Ys):\n","    ax = fig.add_subplot(7, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    num += 1\n","\n","# Now use the classifiers on all datasets\n","for clf in clsfs:\n","    for X, Y in zip(Xs, Ys):\n","        clf.fit(X, Y)\n","        ax = fig.add_subplot(7, 3, num + 1)\n","        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","        colorplot(clf, ax, X[:, 0], X[:, 1])\n","        y_pred = clf.predict(X)\n","        t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","        ax.set_title(t)\n","        num += 1\n","\n","# Note: you may get a FutureWarning, which you can for now just ignore"]},{"cell_type":"code","metadata":{"id":"7J2dqozsrIlN"},"source":["As the datasets used are randomly generated, these plots will look slightly different every time you run this notebook. You can immediately recognize the tree based plots (how?). Besides differences in performance, some classifiers may show a similar performance, but totally different decision boundaries. How do these relate to the type of classifier?"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u3ra2KfDgM0N"},"source":["## Hyperparameters\n","In the previous exercise, we have seen a bunch of different classifiers, which we all initialized with the default parameters. However, all of them, including the random forest, bagging, and voting classifier, include various hyperparameters. These may greatly effect the performance.\n","\n","Let us first look at the parameters of the random forest. The sklearn random forest has 19 (!) different parameters you can set. Here, we will only look into three of these, but you may want to check the others out for the exercises."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sBktnN94gM0O"},"outputs":[],"source":["# Let us first see how much of a difference the number of trees, n_estimators, makes\n","n_trees = [1, 5, 10, 50, 100]\n","\n","# First make plot without classifiers:\n","num = 0\n","fig = plt.figure(figsize=(24,8*len(clsfs)))\n","for X, Y in zip(Xs, Ys):\n","    ax = fig.add_subplot(7, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    num += 1\n","\n","# Now use the classifiers on all datasets\n","for n_tree in n_trees:\n","    for X, Y in zip(Xs, Ys):\n","        clf = RandomForestClassifier(n_estimators=n_tree)\n","        clf.fit(X, Y)\n","        ax = fig.add_subplot(7, 3, num + 1)\n","        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","        colorplot(clf, ax, X[:, 0], X[:, 1])\n","        y_pred = clf.predict(X)\n","        t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","        ax.set_title(t)\n","        num += 1\n"]},{"cell_type":"markdown","metadata":{"id":"QJvhOA0hgM0O"},"source":["With only one tree, you are basically using a decision tree classifier. Multiple trees increases the complexity of the model, effectively giving it more degrees of freedom to use. While this may result in a better decision boundary, this may also result in overfitting. At some point, increasing the number of trees will not substantially alter the results anymore."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgS1RchSgM0O"},"outputs":[],"source":["# Next, let us try to see how bootstrapping influences the process.\n","# Let us first see how much of a difference the number of trees, n_estimators, makes\n","clsfs = [RandomForestClassifier(n_estimators=5, bootstrap=True),\n","         RandomForestClassifier(n_estimators=5, bootstrap=False),\n","         RandomForestClassifier(n_estimators=50, bootstrap=True),\n","         RandomForestClassifier(n_estimators=50, bootstrap=False)]\n","\n","# First make plot without classifiers:\n","num = 0\n","fig = plt.figure(figsize=(24,8*len(clsfs)))\n","for X, Y in zip(Xs, Ys):\n","    ax = fig.add_subplot(7, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    num += 1\n","\n","# Now use the classifiers on all datasets\n","for clf in clsfs:\n","    for X, Y in zip(Xs, Ys):\n","        clf.fit(X, Y)\n","        ax = fig.add_subplot(7, 3, num + 1)\n","        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","        colorplot(clf, ax, X[:, 0], X[:, 1])\n","        y_pred = clf.predict(X)\n","        t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","        ax.set_title(t)\n","        num += 1\n"]},{"cell_type":"markdown","metadata":{"id":"616yW-zwgM0P"},"source":["Both with a small number of trees and a large number, bootstrapping makes quite a difference on the decision boundary, but not neccesary on the performance. How to interpret these results? Would you always use bootstrapping or not?"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"nwaJ7_-1gM0P"},"outputs":[],"source":["# Lastly, if you have an imbalance in your dataset, or one class is more important than the other, you may want\n","# to alter the class weigh in the random forest.\n","clsfs = [RandomForestClassifier(class_weight={0: 1, 1: 0.001}),\n","         RandomForestClassifier(class_weight={0: 1, 1: 1}),\n","         RandomForestClassifier(class_weight={0: 1, 1: 10}),\n","         RandomForestClassifier(class_weight={0: 1, 1: 100})]\n","\n","# First make plot without classifiers:\n","num = 0\n","fig = plt.figure(figsize=(24,8*len(clsfs)))\n","for X, Y in zip(Xs, Ys):\n","    ax = fig.add_subplot(7, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    num += 1\n","\n","# Now use the classifiers on all datasets\n","for clf in clsfs:\n","    for X, Y in zip(Xs, Ys):\n","        clf.fit(X, Y)\n","        ax = fig.add_subplot(7, 3, num + 1)\n","        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","        colorplot(clf, ax, X[:, 0], X[:, 1])\n","        y_pred = clf.predict(X)\n","        t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","        ax.set_title(t)\n","        num += 1\n"]},{"cell_type":"markdown","metadata":{"id":"LoiVM-VqgM0P"},"source":["You can see that according to the class balance, the ratio between the areas wich are classified as 1/0 (blue/red) is changed."]},{"cell_type":"markdown","metadata":{"id":"WeG6neLdgM0P"},"source":["## Feature Importance\n","In the lecture, we have seen that a random forest has a natural form of feature selection and feature importance. Hence, you may use this feature to find out which features have the most predictive value. However, as we also saw, the results may not be straight-forward to interpret, e.g. due to redundant features. Let us therefore create a dataset with 10 features, of which three are actually informative, and two are redundant, and observer the feature importance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2oBjMWcgM0Q"},"outputs":[],"source":["# This example is adapted from https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n","# Build a classification task using 3 informative features\n","X, y = ds.make_classification(n_samples=1000,\n","                           n_features=10,\n","                           n_informative=3,\n","                           n_redundant=2,\n","                           n_repeated=0,\n","                           n_classes=2)\n","\n","# Build a forest and compute the feature importances\n","forest = RandomForestClassifier(n_estimators=100)\n","\n","forest.fit(X, y)\n","importances = forest.feature_importances_\n","std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n","             axis=0)\n","indices = np.argsort(importances)[::-1]\n","\n","# Print the feature ranking\n","print(\"Feature ranking:\")\n","\n","for f in range(X.shape[1]):\n","    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n","\n","# Plot the feature importances of the forest\n","plt.figure()\n","plt.title(\"Feature importances\")\n","plt.bar(range(X.shape[1]), importances[indices],\n","       color=\"r\", yerr=std[indices], align=\"center\")\n","plt.xticks(range(X.shape[1]), indices)\n","plt.xlim([-1, X.shape[1]])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6xjnPn1UgM0Q"},"source":["As the dataset and the random forest include some random effects, your result will change every time you run the code block above. However, you will always observer similar patterns. The plot shows that five of the features contain some importance. It may be able to clearly seperate the three informative from the two redundant features, but in some cases, this might not be obvious. Hence, be careful in usage of this method."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4ELE9JFgM0R"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}