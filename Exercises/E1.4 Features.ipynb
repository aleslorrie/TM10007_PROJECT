{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1YG6LLmlK3IncHGkas3N_KPeXxgH9ha5n","timestamp":1585309933359}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2+"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1QrLGRSB1Xrj"},"source":["# TM10007: Machine learning\n","## Week 2, lecture 2: What is a good feature?\n","#### Author: Hakim C. Achterberg\n","\n","In this exercise, you will have a look at feature handling.\n"]},{"cell_type":"code","metadata":{"id":"xMxkvj70rPMi"},"source":["# General packages\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn import datasets as ds\n","import seaborn\n","\n","\n","# Classifiers\n","from sklearn import model_selection\n","from sklearn import metrics\n","from sklearn import feature_selection\n","from sklearn import preprocessing\n","from sklearn import neighbors\n","from sklearn import svm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ndDvb37erIk8"},"source":["# Some functions we will use\n","def colorplot(clf, ax, x, y, h=100):\n","    '''\n","    Overlay the decision areas as colors in an axes.\n","\n","    Input:\n","        clf: trained classifier\n","        ax: axis to overlay color mesh on\n","        x: feature on x-axis\n","        y: feature on y-axis\n","        h(optional): steps in the mesh\n","    '''\n","    # Create a meshgrid the size of the axis\n","    xstep = (x.max() - x.min() ) / 20.0\n","    ystep = (y.max() - y.min() ) / 20.0\n","    x_min, x_max = x.min() - xstep, x.max() + xstep\n","    y_min, y_max = y.min() - ystep, y.max() + ystep\n","    h = max((x_max - x_min, y_max - y_min))/h\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                         np.arange(y_min, y_max, h))\n","\n","    # Plot the decision boundary. For that, we will assign a color to each\n","    # point in the mesh [x_min, x_max]x[y_min, y_max].\n","    if hasattr(clf, \"decision_function\"):\n","        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n","    else:\n","        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n","    if len(Z.shape) > 1:\n","        Z = Z[:, 1]\n","\n","    # Put the result into a color plot\n","    cm = plt.cm.RdBu_r\n","    Z = Z.reshape(xx.shape)\n","    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n","    del xx, yy, x_min, x_max, y_min, y_max, Z, cm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FrsGY_d91Xr2"},"source":["For illustration of feature scaling, we are considering a dataset with the height and weight of subjects:"]},{"cell_type":"code","metadata":{"id":"h2F8gyQ0rIk-"},"source":["# Create a dataset\n","gender = np.random.randint(0, 2, 500)\n","height = (8 + 5 * gender) * np.random.randn(500) + 160 + 10 * gender\n","bmi = np.random.randn(500) * 1 + 20\n","weight = ((height / 100) ** 2) * bmi\n","\n","dataframe = pd.DataFrame({\n","    'height': height / 100,\n","    'weight': weight,\n","    'bmi': bmi,\n","    'gender': gender,\n","})\n","\n","\n","seaborn.scatterplot(x='height', y='weight', hue='gender', data=dataframe)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BZtEG9Jd1Xr9"},"source":["## Attempt at classification using a kNN\n","\n","Again we will turn to our trusty kNN and see how it can handle the following problem."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"H-JyudUB1Xr-"},"source":["X = np.array(dataframe[['height', 'weight']])\n","y = np.array(dataframe['gender'])\n","\n","clf_knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n","clf_knn.fit(X, y)\n","\n","# Test the classifier on the training data and plot\n","score_train = clf_knn.score(X, y)\n","\n","fig = plt.figure(figsize=(8, 8))\n","ax = fig.add_subplot(1, 1, 1)\n","ax.set_title(f\"Training performance: accuracy {score_train}\")\n","colorplot(clf_knn, ax, X[:, 0], X[:, 1], h=1000)\n","ax.scatter(X[:, 0], X[:, 1], marker='o', c=y,\n","           s=25, edgecolor='k', cmap=plt.cm.Paired)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1e6_o0gn1XsD"},"source":["You can see the classification boundaries being all weird, basically the weight completely dominates the classification and the height has very little influence. This is because the features are not scaled properly."]},{"cell_type":"code","metadata":{"id":"8bsUcyYF1XsE"},"source":["# Scale the dataset\n","scaler = preprocessing.StandardScaler()\n","scaler.fit(X)\n","X_scaled = scaler.transform(X)\n","\n","\n","clf_knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n","clf_knn.fit(X_scaled, y)\n","\n","# Test the classifier on the training data and plot\n","score_train = clf_knn.score(X_scaled, y)\n","\n","fig = plt.figure(figsize=(8, 8))\n","ax = fig.add_subplot(1, 1, 1)\n","ax.set_title(f\"Training performance: accuracy {score_train}\")\n","colorplot(clf_knn, ax, X_scaled[:, 0],X_scaled[:, 1], h=1000)\n","ax.scatter(X_scaled[:, 0], X_scaled[:, 1], marker='o', c=y,\n","           s=25, edgecolor='k', cmap=plt.cm.Paired)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1WhxUubO1XsI"},"source":["### Exercise: Different scaling method\n","\n","This is the most common scaling method. But there are other methods, such as the min-max scaling using the `sklearn.preprocessing.MinMaxScaler` or `sklearn.preprocessing.RobustScaler`. Try adapt the above example and look at the difference. When would you use min-max and when would you use standard scaling?"]},{"cell_type":"markdown","metadata":{"id":"ZS-D26q51XsJ"},"source":["# Feature selection/extraction\n","\n","When we have very high dimensional data, we can use feature selection or extraction to make the problem easier for a classifier."]},{"cell_type":"code","metadata":{"id":"5H1CeAiL1XsK"},"source":["X2, y2 = ds.make_classification(n_samples=250, n_features=50, n_informative=5,\n","                              n_redundant=20, n_repeated=10, n_classes=2,\n","                              n_clusters_per_class=2, class_sep=0.7, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wLjOg57H1XsO"},"source":["Use recursive feature elimination to find a good amount of features. This implementation use the importance of features in a classifier to eliminate features."]},{"cell_type":"code","metadata":{"id":"JKuJanv11XsP"},"source":["# Create the RFE object and compute a cross-validated score.\n","svc = svm.SVC(kernel=\"linear\")\n","\n","# classifications\n","rfecv = feature_selection.RFECV(\n","    estimator=svc, step=1,\n","    cv=model_selection.StratifiedKFold(4),\n","    scoring='roc_auc')\n","rfecv.fit(X2, y2)\n","\n","# Plot number of features VS. cross-validation scores\n","plt.figure()\n","plt.xlabel(\"Number of features selected\")\n","plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n","plt.plot(range(1, len(rfecv.cv_results_[\"mean_test_score\"]) + 1), rfecv.cv_results_[\"mean_test_score\"])\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uDSL9ytw1XsT"},"source":["## PCA\n","\n","Principal component analysis allows you to rotate the data so that the first component captures to most variance, and the second the second most, etc."]},{"cell_type":"code","metadata":{"id":"dWAEOE7h1XsU"},"source":["seaborn.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNKNaFxZ1XsX"},"source":["from sklearn import decomposition\n","\n","pca = decomposition.PCA(n_components=2)\n","pca.fit(X_scaled)\n","X_pca = pca.transform(X_scaled)\n","\n","seaborn.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dHelRIX1Xsb"},"source":["### Question\n","\n","Does a PCA where you retain all component help in the classification when using a kNN classfier? And what about a Random Forest classifier?"]},{"cell_type":"markdown","metadata":{"id":"jj8XNQoy1Xsc"},"source":["# Putting things together\n","\n","It is important to remember that all parts of your model will be only fit on the training data"]},{"cell_type":"code","metadata":{"id":"ULpAoUSl1Xsd"},"source":["# Split data\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y)\n","\n","# Scale the data to be normal\n","scaler = preprocessing.StandardScaler()\n","scaler.fit(X_train)\n","X_train_scaled = scaler.transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Perform a PCA\n","pca = decomposition.PCA(n_components=2)\n","pca.fit(X_train_scaled)\n","X_train_pca = pca.transform(X_train_scaled)\n","X_test_pca = pca.transform(X_test_scaled)\n","\n","# Fit kNN\n","knn = neighbors.KNeighborsClassifier(n_neighbors=15)\n","knn.fit(X_train_pca, y_train)\n","score_train = knn.score(X_train_pca, y_train)\n","score_test = knn.score(X_test_pca, y_test)\n","\n","# Print result\n","print(f\"Training result: {score_train}\")\n","print(f\"Test result: {score_test}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44Tdlglc1Xsg"},"source":["As you can see the steps are always fit only on the train data, where the transforns are applied to the train and test data and then moved to the next step."]}]}