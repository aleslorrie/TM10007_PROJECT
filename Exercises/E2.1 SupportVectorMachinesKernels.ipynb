{"cells":[{"cell_type":"markdown","metadata":{"id":"3BN9-1yZffNT"},"source":["# TM10007: Machine learning\n","## Week 3, lecture 1: Support Vector Machines and Kernels\n","#### Author: Martijn P. A. Starmans\n","\n","In this exercise, you will learn how to use support vector machines and kernels using scikit learn.\n","\n","For more of these methods, visit https://scikit-learn.org/stable/modules/svm html, https://scikit-learn.org/stable/modules/kernel_approximation.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMxkvj70rPMi"},"outputs":[],"source":["# General packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets as ds\n","\n","# Classifiers and kernels\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import SVC\n","from sklearn.decomposition import PCA, KernelPCA\n","from sklearn.kernel_approximation import RBFSampler\n","from sklearn.metrics.pairwise import rbf_kernel, sigmoid_kernel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ndDvb37erIk8"},"outputs":[],"source":["# Some functions we will use\n","def colorplot(clf, ax, x, y, h=100, precomputer=None):\n","    '''\n","    Overlay the decision areas as colors in an axes.\n","\n","    Input:\n","        clf: trained classifier\n","        ax: axis to overlay color mesh on\n","        x: feature on x-axis\n","        y: feature on y-axis\n","        h(optional): steps in the mesh\n","    '''\n","    # Create a meshgrid the size of the axis\n","    xstep = (x.max() - x.min() ) / 20.0\n","    ystep = (y.max() - y.min() ) / 20.0\n","    x_min, x_max = x.min() - xstep, x.max() + xstep\n","    y_min, y_max = y.min() - ystep, y.max() + ystep\n","    h = max((x_max - x_min, y_max - y_min))/h\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                         np.arange(y_min, y_max, h))\n","\n","    features = np.c_[xx.ravel(), yy.ravel()]\n","    if precomputer is not None:\n","        if type(precomputer) is RBFSampler:\n","            features = precomputer.transform(features)\n","        elif precomputer is rbf_kernel:\n","            features = rbf_kernel(features, X)\n","\n","    # Plot the decision boundary. For that, we will assign a color to each\n","    # point in the mesh [x_min, x_max]x[y_min, y_max].\n","    if hasattr(clf, \"decision_function\"):\n","        Z = clf.decision_function(features)\n","    else:\n","        Z = clf.predict_proba(features)\n","    if len(Z.shape) > 1:\n","        Z = Z[:, 1]\n","\n","    # Put the result into a color plot\n","    cm = plt.cm.RdBu_r\n","    Z = Z.reshape(xx.shape)\n","    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n","    del xx, yy, x_min, x_max, y_min, y_max, Z, cm\n","\n","def load_breast_cancer(n_features=2):\n","    '''\n","    Load the sklearn breast data set, but reduce the number of features with PCA.\n","    '''\n","    data = ds.load_breast_cancer()\n","    x = data['data']\n","    y = data['target']\n","\n","    p = PCA(n_components=n_features)\n","    p = p.fit(x)\n","    x = p.transform(x)\n","    return x, y\n","\n","def load_boston(n_features=1):\n","    '''\n","    Load the sklearn boston data set, but reduce the number of features with PCA.\n","    '''\n","    data = ds.load_boston()\n","    x = data['data']\n","    y = data['target']\n","\n","    p = PCA(n_components=n_features)\n","    p = p.fit(x)\n","    x = p.transform(x)\n","    return x, y\n","\n","def load_diabetes(n_features=1):\n","    '''\n","    Load the sklearn bdiabetes data set, but reduce the number of features with PCA.\n","    '''\n","    data = ds.load_diabetes()\n","    x = data['data']\n","    y = data['target']\n","\n","    p = PCA(n_components=n_features)\n","    p = p.fit(x)\n","    x = p.transform(x)\n","    return x, y"]},{"cell_type":"markdown","metadata":{"id":"qtVwgmjiffNY"},"source":["Let us first create again three example datasets to play with and plot the feature distributions in scatter plots."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2F8gyQ0rIk-"},"outputs":[],"source":["# Load and plot three different classification datasets\n","X2, Y2 = ds.make_classification(n_samples=100, n_features=2, n_redundant=0,\n","                                n_informative=1,\n","                                n_clusters_per_class=1)\n","fig = plt.figure(figsize=(24,8))\n","ax = fig.add_subplot(131)\n","ax.set_title(\"One informative feature, one cluster per class\", fontsize='small')\n","ax.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","\n","X3, Y3 = ds.make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=5)\n","ax = fig.add_subplot(132)\n","ax.set_title(\"Two blobs, two classes\", fontsize='small')\n","ax.scatter(X3[:, 0], X3[:, 1], marker='o', c=Y3,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","\n","X4, Y4 = load_breast_cancer()\n","ax = fig.add_subplot(133)\n","ax.set_title(\"A more complicated problem\", fontsize='small')\n","ax.scatter(X4[:, 0], X4[:, 1], marker='o', c=Y4,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)"]},{"cell_type":"markdown","source":["Previously, we have seen how sklearn can be used to fit and test a classifier on these datasets using both decision trees and a nearest neighbors classifiers. Now, we are going to see how we can use and evaluate\n","support vector machine and kernel approaches\n","\n","**Support Vector Machines**\n","Altough Support Vector Machines are quite intuitive and easy to use, they have quite a lot of hyperparameters. One general and highly important parameter is the slack C, which defaults to 1.0 in scikit learn. The SVM in scikit-learn contains defaults for the linear, polynomial and radial basis function kernels, and has options for their specific hyperparameters. However, SVMs also accept manually constructed and precomputed kernels.\n","\n","**Kernels**\n","Kernels can be used to transform a feature space to another, often higher dimensional feature space. When the classes are more easily separable in the new feature space, this transformation may help any classifier to perform better. Scikit learn offers a variety of kernels, which we as stated can use with any classifiers. However, as the SVM mathematically is highly suitable to be used with kernels, only the SVM contains a nice interaction with kernels.\n","\n","Let's start with using some default SVMs, than experiment with some hyperparameters, and finally use some alternative kernels with SVMs, but also with other kernels."],"metadata":{"id":"ngZGS0Wih_ik"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5yVUZZ_hrIlF"},"outputs":[],"source":["# Construct classifiers\n","svmlin = SVC(kernel='linear', gamma='scale')\n","svmrbf = SVC(kernel='rbf', gamma='scale')\n","svmpoly = SVC(kernel='poly', degree=3, gamma='scale')\n","\n","clsfs = [KNeighborsClassifier(), RandomForestClassifier(), svmlin, svmpoly, svmrbf]\n","\n","\n","# Create lists of datasets to loop over\n","Xs = [X2, X3, X4]\n","Ys = [Y2, Y3, Y4]\n","\n","# First make plot without classifiers:\n","num = 0\n","fig = plt.figure(figsize=(24,8*len(clsfs)))\n","for X, Y in zip(Xs, Ys):\n","    ax = fig.add_subplot(7, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    num += 1\n","\n","# Now use the classifiers on all datasets\n","for clf in clsfs:\n","    for X, Y in zip(Xs, Ys):\n","        clf.fit(X, Y)\n","        ax = fig.add_subplot(7, 3, num + 1)\n","        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","        colorplot(clf, ax, X[:, 0], X[:, 1])\n","        y_pred = clf.predict(X)\n","        t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","        ax.set_title(t)\n","        num += 1\n","\n","# Note: you may get a FutureWarning, which you can for now just ignore"]},{"cell_type":"markdown","source":["You can immediately see that the decision boundaries created by the SVMs look very different from a Random Forest or KNN. Also, you can also easily recognize the type of kernel used in the SVM from the shapes of the decision boundaries. Can you see how?\n","\n","Probably, your SVMs performed far worse than the other classifiers, except from maybe the first dataset. However, keep in mind that these classifiers are now both trained and tested on the same dataset. Which classifier would you pick to test on an external test dataset, i.e. which classifier do you think will generalize better?\n","\n","The importance of tuning your model complexity to the application will be addressed further in the next lecture."],"metadata":{"id":"RZ3YV2v7ioRq"}},{"cell_type":"markdown","source":["## SVM hyperparameters\n","As stated, the SVM contains quite some hyperparameters. Let us for know focus on the polynomial SVM, which contains key hyperparameters:\n","- The degree of the kernel\n","- The homogeneity of the kernel, called coef0 in sklearn. What does this parameter actually do?\n","- The general slack parameter\n","\n","Do you think it is nice to have such a large amount of hyperparemeters, or not?\n","\n","Note that there are far more, check the scikit-learn documentation. Let us for now see how these parameters influence our performance.\n","\n","Let us run the same experiment, but varying these hyperparameters."],"metadata":{"id":"TqvQPu9SisfF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNMw4XoMrIlL"},"outputs":[],"source":["# Construct classifiers\n","degrees = [1, 3, 5]\n","coef0s = [0.01, 0.5, 1]\n","slacks = [0.01, 0.5, 1]\n","\n","clsfs = list()\n","for degree in degrees:\n","    for coef0 in coef0s:\n","        for slack in slacks:\n","            clsfs.append(SVC(kernel='poly', degree=degree, coef0=coef0, C=slack, gamma='scale'))\n","\n","# First make plot without classifiers:\n","num = 0\n","fig = plt.figure(figsize=(24,8*len(clsfs)))\n","for X, Y in zip(Xs, Ys):\n","    ax = fig.add_subplot(len(clsfs) + 1, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    num += 1\n","\n","# Now use the classifiers on all datasets\n","for clf in clsfs:\n","    for X, Y in zip(Xs, Ys):\n","        clf.fit(X, Y)\n","        ax = fig.add_subplot(len(clsfs) + 1, 3, num + 1)\n","        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","            s=25, edgecolor='k', cmap=plt.cm.Paired)\n","        colorplot(clf, ax, X[:, 0], X[:, 1])\n","        y_pred = clf.predict(X)\n","        t = f\"degree: {clf.degree}, coef0: {clf.coef0}, C: {clf.C}. \"\n","        t = t + (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","        ax.set_title(t)\n","        num += 1\n","\n","# Note: you may get a FutureWarning, which you can for now just ignore"]},{"cell_type":"markdown","source":["What do you observe:\n","- What is the influence of coef0, the slack, and the degree on the boundaries?\n","- Does giving the SVM more freedom (i.e. higher degree, higher slack) results in better results?\n","- Which solution would you pick to externally validate? Would you pick the same on each dataset?\n","- For many classifiers, the performance is actually the similar: most of your classifiers will have a perfect score on the first dataset. However, the decision boundaries are quite different. How could you determine automatically which one is best?\n","\n","Again, note that the SVM has more hyperparameters and functionalities as explained here, which may be useful for the final assignment."],"metadata":{"id":"Kh570vhwi4_b"}},{"cell_type":"markdown","metadata":{"id":"Tf973_EPffNc"},"source":["## Kernels\n","We have just seen how the SVM in scikit-learn already has some kernels implemented. However, there exist many more kernels, and kernels can be applied in combination with other estimators as well, which is what we are going to look at now.\n","\n","Let us first take a closer look at the radial basis function (RBF) kernel, which is similar to a Gaussian, and was already included in the SVM. Scikit-learn offers three alternatives for using such a kernel: 1) a sampler, which transforms your feature space using such a kernel and randomly draws some feature samples from this space; 2) a kernel function, which we can use inside the SVM; or 3) precomputing the kernel\n","\n","RBFSampler\n","gaussian_process.kernels.RBF(\n","also use a weird kernel, e.g. ExpSineSquared\n","compare with RBF SVM, RandomForest on kernel features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWu6qEGaffNd"},"outputs":[],"source":["# First make plot without classifiers:\n","num = 0\n","fig = plt.figure(figsize=(24,8*3))\n","for X, Y in zip(Xs, Ys):\n","    ax = fig.add_subplot(5 + 1, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    num += 1\n","\n","# Option 1: use the standard RBF kernel from the scikit-learn SVM\n","clf = SVC(kernel='rbf', gamma=1)\n","for X, Y in zip(Xs, Ys):\n","    clf.fit(X, Y)\n","    ax = fig.add_subplot(5 + 1, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    colorplot(clf, ax, X[:, 0], X[:, 1])\n","    y_pred = clf.predict(X)\n","    t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","    ax.set_title(t)\n","    num += 1\n","\n","\n","# Option 2: Manually transform the feature space with an RBF kernel, sample a part of this, than apply a linear SVM\n","rbf_feature = RBFSampler(gamma=1, random_state=1)\n","clf = SVC(kernel='linear')\n","for X, Y in zip(Xs, Ys):\n","    XR = rbf_feature.fit_transform(X)\n","    clf.fit(XR, Y)\n","    ax = fig.add_subplot(5 + 1, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    colorplot(clf, ax, X[:, 0], X[:, 1], precomputer=rbf_feature)\n","    y_pred = clf.predict(XR)\n","    t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","    ax.set_title(t)\n","    num += 1\n","\n","# Option 3: supply a manually constructed kernel function to the SVM\n","clf = SVC(kernel=rbf_kernel)\n","for X, Y in zip(Xs, Ys):\n","    clf.fit(X, Y)\n","    ax = fig.add_subplot(5 + 1, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    colorplot(clf, ax, X[:, 0], X[:, 1])\n","    y_pred = clf.predict(X)\n","    t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","    ax.set_title(t)\n","    num += 1\n","\n","# Option 4: precompute the RBF kernel\n","clf = SVC(kernel='precomputed')\n","for X, Y in zip(Xs, Ys):\n","    rbf_kernel_computed = rbf_kernel(X)\n","    clf.fit(rbf_kernel_computed, Y)\n","    ax = fig.add_subplot(5 + 1, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    colorplot(clf, ax, X[:, 0], X[:, 1], precomputer=rbf_kernel)\n","    y_pred = clf.predict(rbf_kernel_computed)\n","    t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","    ax.set_title(t)\n","    num += 1\n","\n","# Extra: use a totally different kernel!\n","clf = SVC(kernel=sigmoid_kernel)\n","for X, Y in zip(Xs, Ys):\n","    clf.fit(X, Y)\n","    ax = fig.add_subplot(5 + 1, 3, num + 1)\n","    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n","        s=25, edgecolor='k', cmap=plt.cm.Paired)\n","    colorplot(clf, ax, X[:, 0], X[:, 1])\n","    y_pred = clf.predict(X)\n","    t = (\"Misclassified: %d / %d\" % ((Y != y_pred).sum(), X.shape[0]))\n","    ax.set_title(t)\n","    num += 1"]},{"cell_type":"markdown","metadata":{"id":"C-cBt57yffNd"},"source":["What do you observe:\n","- What happens when we subsample the RBF space?\n","- What happens if we supply a different RBF kernel?\n","- What happens if we precompute the features? Hint: check the training time!\n","- How did the last kernel do?"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":0}