{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjQu_kqeoLW0"
   },
   "source": [
    "TM10007: Machine learning\n",
    "Week 1, lecture 1: Introduction to ClassifiersÂ¶\n",
    "In this exercise, you will learn how to use different classifiers using scikit learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4avbCGwnJQ3"
   },
   "outputs": [],
   "source": [
    "# General packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets as ds\n",
    "from sklearn import metrics\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsdHtRm7n9B8"
   },
   "outputs": [],
   "source": [
    "# some function that we will use\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def colorplot(clf, ax, x, y, h=100, precomputer=None):\n",
    "    '''\n",
    "    Overlay the decision areas as colors in an axes.\n",
    "\n",
    "    Input:\n",
    "        clf: trained classifier\n",
    "        ax: axis to overlay color mesh on\n",
    "        x: feature on x-axis\n",
    "        y: feature on y-axis\n",
    "        h(optional): steps in the mesh\n",
    "    '''\n",
    "    # Create a meshgrid the size of the axis\n",
    "    xstep = (x.max() - x.min() ) / 20.0\n",
    "    ystep = (y.max() - y.min() ) / 20.0\n",
    "    x_min, x_max = x.min() - xstep, x.max() + xstep\n",
    "    y_min, y_max = y.min() - ystep, y.max() + ystep\n",
    "    h = max((x_max - x_min, y_max - y_min))/h\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    features = np.c_[xx.ravel(), yy.ravel()]\n",
    "    if precomputer is not None:\n",
    "        if type(precomputer) is RBFSampler:\n",
    "            features = precomputer.transform(features)\n",
    "        elif precomputer is rbf_kernel:\n",
    "            features = rbf_kernel(features, X)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(features)\n",
    "    else:\n",
    "        Z = clf.predict_proba(features)\n",
    "    if len(Z.shape) > 1:\n",
    "        Z = Z[:, 1]\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    cm = plt.cm.RdBu_r\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "    del xx, yy, x_min, x_max, y_min, y_max, Z, cm\n",
    "\n",
    "def load_breast_cancer(n_features=2):\n",
    "    '''\n",
    "    Load the sklearn breast data set, but reduce the number of features with PCA.\n",
    "    '''\n",
    "    data = ds.load_breast_cancer()\n",
    "    x = data['data']\n",
    "    y = data['target']\n",
    "\n",
    "    p = PCA(n_components=n_features)\n",
    "    p = p.fit(x)\n",
    "    x = p.transform(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD9C8aNzxncl"
   },
   "source": [
    "We will create a dataset and then classify it with a linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thHSY0FkqZLC"
   },
   "outputs": [],
   "source": [
    "X1, Y1 = ds.make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
    "                                n_informative=2,\n",
    "                                n_clusters_per_class=1)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title(\"Two informative features, one cluster per class\",\n",
    "             fontsize='small')\n",
    "ax.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "           s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda = lda.fit(X1, Y1)\n",
    "y_pred = lda.predict(X1)\n",
    "colorplot(lda, ax, X1[:, 0], X1[:, 1])\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (X1.shape[0], (Y1 != y_pred).sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fte22rOPxKQr"
   },
   "source": [
    "Create three example datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39SVvDYRsaiP"
   },
   "outputs": [],
   "source": [
    "X2, Y2 = ds.make_classification(n_samples=100, n_features=2, n_redundant=0,\n",
    "                                n_informative=1,\n",
    "                                n_clusters_per_class=1)\n",
    "fig = plt.figure(figsize=(24, 8))\n",
    "ax = fig.add_subplot(131)\n",
    "ax.set_title(\"One informative feature, one cluster per class\", fontsize='small')\n",
    "ax.scatter(X2[:, 0], X2[:, 1], marker='o', c=Y2,\n",
    "           s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "X3, Y3 = ds.make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=5)\n",
    "ax = fig.add_subplot(132)\n",
    "ax.set_title(\"Two blobs, two classes\", fontsize='small')\n",
    "ax.scatter(X3[:, 0], X3[:, 1], marker='o', c=Y3, s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n",
    "X4, Y4 = load_breast_cancer()\n",
    "ax = fig.add_subplot(133)\n",
    "ax.set_title(\"A more complicated problem\", fontsize='small')\n",
    "ax.scatter(X4[:, 0], X4[:, 1], marker='o', c=Y4, s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2Wla57uy2Qb"
   },
   "source": [
    "Try out the different classifiers with the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fO5NAzowtWzb"
   },
   "outputs": [],
   "source": [
    "#   - GaussianNB\n",
    "#   - LinearDiscriminantAnalysis\n",
    "#   - QuadraticDiscriminantAnalysis\n",
    "#   - LogisticRegression\n",
    "#   - SGDClassifier\n",
    "#   - KNeighborsClassifier\n",
    "#   Motivate your choice. You can use the example code below to loop over both\n",
    "#   the datasets and the classifiers at the same time:\n",
    "\n",
    "\n",
    "clsfs = [LinearDiscriminantAnalysis(),QuadraticDiscriminantAnalysis(),GaussianNB(),\n",
    "         LogisticRegression(),SGDClassifier(),KNeighborsClassifier()]\n",
    "Xs = [X2, X3, X4]\n",
    "Ys = [Y2, Y3, Y4]\n",
    "clfs_fit = list()\n",
    "\n",
    "# First make a plot without classifiers:\n",
    "fig = plt.figure(figsize=(21,7*len(clsfs)))\n",
    "num = 0  # Iteration number for the subplots\n",
    "for X, Y in zip(Xs, Ys):\n",
    "    ax = fig.add_subplot(6, 3, num + 1)\n",
    "    ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n",
    "               s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "    num += 1\n",
    "\n",
    "# Fit the classifiers and add them to the plot\n",
    "num=0\n",
    "Xt=list()\n",
    "Yt=list()\n",
    "for clf in clsfs:\n",
    "    for X, Y in zip(Xs, Ys):\n",
    "        # Fit classifier\n",
    "        clf.fit(X,Y)\n",
    "        y_pred=clf.predict(X)\n",
    "        # Predict labels using fitted classifier\n",
    "\n",
    "        # Make scatterplot of features\n",
    "        ax = fig.add_subplot(6, 3, num + 1)\n",
    "        ax.scatter(X[:, 0], X[:, 1], marker='o', c=Y,\n",
    "               s=25, edgecolor='k', cmap=plt.cm.Paired)\n",
    "        colorplot(clf, ax, X[:,0], X[:,1])\n",
    "        # Add overlay through colorplot function\n",
    "        t=(\"Misclass: %d / %d\" % ((Y!=y_pred).sum(), X.shape[0]))\n",
    "        ax.set_title(t)\n",
    "        num+=1\n",
    "\n",
    "        clfs_fit.append(clf)\n",
    "        Xt.append(X)\n",
    "        Yt.append(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w48HbhYIYzht"
   },
   "source": [
    "We have already identified what you should do in the loop:\n",
    " 1. Fit the classifier on the features.\n",
    " 2. Predict the labels on the features from the same dataset.\n",
    " 3. Make a scatterplot of the features and label the points  with their\n",
    "    true labels (not the predicted ones) as we did above. Use the colorplot function to overlay the decision boundary.\n",
    " 4. Put the number of misclassified samples in the title of the plot.\n",
    "Plot the results of all classifiers on a single dataset in a single figure. The result will thus be three figures, each with six subplots. Play around with the different parameters for each classifier and pick some values that you think work best in each case. Motivate your choices.\n",
    "\n",
    "Note: the KNeighborsClassifier does not need the labels!\n",
    "\n",
    "2. Which classifier works best on which dataset? Why? Can you deduce from the scatterplot of the features which classifier might work best?\n",
    "\n",
    "3. Also try the DecisionTreeClassifier classifier and make the same plots.\n",
    "This classifier will perform better than the other ones. However, looking at the colorplot, you will see that it overfits on this dataset. The\n",
    "decision boundary will not generalize well if we add other samples.\n",
    "\n",
    "Picking the right classifier for a problem is a challenging task.\n",
    "The distribution of the features and the complexity of the problem\n",
    "may give you some directions. However, proper visualization of the\n",
    "features distribution becomes difficult when the number of features\n",
    "increases. Aditionally, we are now dealing with binary classification\n",
    "problems. Multiclass problems add another layer of complexity.\n",
    "Some of the classifiers you used can be used for multiclass problems,\n",
    "but some cannot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Zib9U82aDFj"
   },
   "source": [
    "You can now run some basic classifiers and visualize the results. This\n",
    "gives you a qualitative performance of your method. We would like\n",
    "to get a more quantitative result in most cases. Additionally, when using\n",
    "more than two features, the decision boundary results are difficult\n",
    "to visualize. Let us therefore look at some quantitative measures to\n",
    "evaluate performance. Let's first again train a classifier on a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UE4nIYSRaKda"
   },
   "outputs": [],
   "source": [
    "\n",
    "X1, y_truth = load_breast_cancer()\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(X1, y_truth)\n",
    "y_pred = gnb.predict(X1)\n",
    "\n",
    "# In https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameters\n",
    "# you can find the metric that you want to use:\n",
    "#   - Accuracy\n",
    "#   - AUC\n",
    "#   - F1-score\n",
    "#   - precision\n",
    "#   - recall\n",
    "#\n",
    "#   Compute the five metrics on your classification results from the former exercise\n",
    "#   Print the results to the console. Do these agree with the conclusions\n",
    "#   you drew from this exercise about which classifier works best?\n",
    "#   Which metric(s) do you think is (are) most most important?\n",
    "#\n",
    "#   Note2: For the AUC score, you do not want the binary predictions, but\n",
    "#          rather the scores between 0 and 1. Not all classifiers can\n",
    "#          however give such a so called \"posterior\". You can check whether a\n",
    "#          classifier clf can give the scores and otherwise use the labels\n",
    "#          by using the following:\n",
    "\n",
    "for clf, X1, Y1 in zip(clfs_fit, Xt, Yt):\n",
    "    y_pred=clf.predict(X1)\n",
    "\n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "    # The first column gives the probability for class = 0, so we take\n",
    "    # the second which gives the probability class = 1:\n",
    "        y_score = clf.predict_proba(X1)[:, 1]\n",
    "    else:\n",
    "       y_score = y_pred\n",
    "\n",
    "# The hasattr function checks whether an object, function or package has\n",
    "# a certain attribute. This attribute can be a subfunction, or again an\n",
    "# object or function, but also things like scalars or strings.\n",
    "\n",
    "    auc=metrics.roc_auc_score(Y1, y_score)\n",
    "    accuracy=metrics.accuracy_score(Y1, y_pred)\n",
    "    F1=metrics.f1_score(Y1,y_pred)\n",
    "    precision=metrics.precision_score(Y1,y_pred)\n",
    "    recall=metrics.recall_score(Y1, y_pred)\n",
    "# accuracy, AUC, f1score, precision, recall\n",
    "    print(type(clf))\n",
    "    print('Acc:' +str(accuracy))\n",
    "    print('AUC:' +str(auc))\n",
    "    print('F1:' +str(F1))\n",
    "    print('precision:' +str(precision))\n",
    "    print('recall:' +str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_hq1Tq0uCh_"
   },
   "source": [
    "We previously loaded only a subset of a breast cancer dataset.\n",
    "Load the full sklearn breast cancer data set with te following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jj_L5rRRuK96"
   },
   "outputs": [],
   "source": [
    "data = ds.load_breast_cancer()\n",
    "X = data['data']\n",
    "Y = data['target']\n",
    "\n",
    "#   The resulting data object is a dictionary, containing not only the feature\n",
    "#   values (in the \"data\" field) and the binary labels (in the \"target\") field,\n",
    "#   but also a.o. the feature names. There are now 30\n",
    "#   features per patient. Use the top three classifiers from part 2 of this\n",
    "#   assignment to reclassify the patients and compute the five evaluation\n",
    "#   metrics. Print the results to the console. Do these classifiers\n",
    "#   also perform well on this dataset?\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOQIpSUVAtL6dKhhSyCjuWo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
